{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Notes 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 1.0 - General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import keras\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Input, Conv2D, Flatten, MaxPooling2D, LSTM\n",
    "from keras.models import Sequential, model_from_json, Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "\n",
    "from keras.datasets import mnist, cifar10\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivation(values):\n",
    "    return values\n",
    "\n",
    "def stepFunction(values):\n",
    "    if values > 1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def sigmoidActivation(values):\n",
    "    return 1 / (1 + np.exp(-1 * values))\n",
    "\n",
    "def tanhActivation(values):\n",
    "    return (np.exp(values) - np.exp(-values)) / (np.exp(values) + np.exp(-1 * values))\n",
    "\n",
    "def reLU(values):\n",
    "    if values >= 0:\n",
    "        return values\n",
    "    return 0\n",
    "\n",
    "def leakyReLU(values, leaky=0.01):\n",
    "    if values >= 0:\n",
    "        return values\n",
    "    return leaky * values\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x)\n",
    "    return ex/ex.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(erros):\n",
    "    return (1/len(erros)) * math.sqrt(sum([abs(e) for e in erros]))\n",
    "\n",
    "def MSE(erros):\n",
    "    return (1/len(erros)) * sum([e**2 for e in erros])\n",
    "\n",
    "def RMSE(erros):\n",
    "    return math.sqrt((1/len(erros)) * sum([e**2 for e in erros]))\n",
    "\n",
    "def accuracy(tp, tn, total):\n",
    "    return (tp + tn)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_0 = (5*2 + 2*5 + 1*1)/10\n",
    "_1 = np.array([5.0, 2.0, 1.3])\n",
    "_2 = [1 - 0.3, 0 - 0.2, 1 - 0.89, 0 - 0.32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    f'Linear: {linearActivation(_0)}',\n",
    "    f'Step: {stepFunction(_0)}',\n",
    "    f'Sigmoid: {sigmoidActivation(_0)}',\n",
    "    f'TanH: {tanhActivation(_0)}',\n",
    "    f'ReLU: {reLU(_0)}',\n",
    "    f'Leaky ReLU:{leakyReLU(_0)}',\n",
    "    f'Softmax: {softmax(_1)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    f'MAE: {MAE(_2)}',\n",
    "    f'MSE: {MSE(_2)}',\n",
    "    f'RMSE: {RMSE(_2)}',\n",
    "    f'Acc: {accuracy(1, 2, 4)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nans(df):\n",
    "    return df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 2.0 - Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/breast_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv('datasets/breast_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(df, classes,\n",
    "                                                test_size=0.25,\n",
    "                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'trainX: {trainX.shape}, trainY:{trainY.shape}, \\\n",
    "testX: {testX.shape}, testY: {testY.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=30,\n",
    "                activation='relu',\n",
    "                kernel_initializer='random_uniform',\n",
    "                input_dim=30,\n",
    "                name='layer_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=30,\n",
    "                activation='relu',\n",
    "                kernel_initializer='random_uniform',\n",
    "                name='layer_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=10,\n",
    "                activation='relu',\n",
    "                kernel_initializer='random_uniform',\n",
    "                name='layer_3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=10,\n",
    "                activation='relu',\n",
    "                kernel_initializer='random_uniform',\n",
    "                name='layer_4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1, activation='sigmoid', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = keras.optimizers.Adam(lr=0.01, decay=0.0001, clipvalue=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.compile(optimizer=my_optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=trainX, y=trainY, epochs=100, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x=testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x=testX, y=testY)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(testY, pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(testY, pred)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(testY, pred)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(testY, pred)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, b0 = model.layers[0].get_weights()[0], model.layers[0].get_weights()[1]\n",
    "w1, b1 = model.layers[1].get_weights()[0], model.layers[1].get_weights()[1]\n",
    "w1, b1 = model.layers[1].get_weights()[0], model.layers[1].get_weights()[1]\n",
    "w2, b2 = model.layers[2].get_weights()[0], model.layers[2].get_weights()[1]\n",
    "w3, b3 = model.layers[3].get_weights()[0], model.layers[3].get_weights()[1]\n",
    "w4, b4 = model.layers[4].get_weights()[0], model.layers[4].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0.shape, b0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3.shape, b3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w4.shape, b4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 1.1 - Predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.array(\n",
    "    [\n",
    "        [\n",
    "            15.8, 8.34, 118, 900, 0.10, 0.26, 0.08, 0.134, 0.178, 0.20,\n",
    "            0.05, 1098, 0.87, 4500, 145.2, 0.005, 0.04, 0.05, 0.014, 0.03,\n",
    "            0.007, 23.15, 16.63, 178.5, 2018, 0.14, 0.185, 0.84, 158, 0.363\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# since sigmoide is probabilistic, they'll have the same values\n",
    "f'pred: {model.predict(x=new_data)[0][0]}, pred_proba: {model.predict_proba(x=new_data)[0][0]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 1.2 - Export the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_breast.json', mode='w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_breast.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -laH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 1.3 - Import the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model_str = None\n",
    "with open('model_breast.json', mode='r') as f:\n",
    "    json_model_str = f.read()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model = model_from_json(json_string=json_model_str)\n",
    "json_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model.load_weights('model_breast.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model.predict(x=new_data)[0][0], json_model.predict_proba(x=new_data)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model.evaluate(x=df, y=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 2.1 - K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetowrk():\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(units=30, activation='relu',kernel_initializer='random_uniform', input_dim=30))\n",
    "    clf.add(Dropout(rate=0.2, seed=0))\n",
    "    clf.add(Dense(units=30, activation='relu', kernel_initializer='random_uniform'))\n",
    "    clf.add(Dropout(rate=0.2, seed=0))\n",
    "    clf.add(Dense(units=30, activation='relu', kernel_initializer='random_uniform'))\n",
    "    clf.add(Dropout(rate=0.2, seed=0))\n",
    "    clf.add(Dense(units=30, activation='relu', kernel_initializer='random_uniform'))\n",
    "    clf.add(Dense(units=1, activation='sigmoid'))\n",
    "    optimizer = keras.optimizers.Adam(lr=0.01, decay=0.0001, clipvalue=0.5)\n",
    "    clf.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifier(build_fn=createNetowrk, epochs=100, batch_size=10)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "res = cross_val_score(estimator=clf, X=df, y=classes, cv=kfold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.mean(), res.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 2.2 - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetowrkGridSearch(units, dropout, optimizer, loss, activation):\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(input_dim=30, units=units, activation=activation,\n",
    "                  kernel_initializer='random_uniform', name='dense_1'))\n",
    "    clf.add(Dropout(rate=dropout, seed=0, name='dropout_1'))\n",
    "    clf.add(Dense(units=units, activation=activation,\n",
    "                  kernel_initializer='random_uniform', name='dense_2'))\n",
    "    clf.add(Dropout(rate=dropout, seed=0, name='dropout_2'))\n",
    "    clf.add(Dense(units=units, activation=activation,\n",
    "                  kernel_initializer='random_uniform', name='dense_3'))\n",
    "    clf.add(Dropout(rate=dropout, seed=0, name='dropout_3'))\n",
    "    clf.add(Dense(units=units, activation=activation,\n",
    "                  kernel_initializer='random_uniform', name='dense_4'))\n",
    "    clf.add(Dense(units=1, activation='sigmoid', name='output'))\n",
    "    clf.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gridsearch = KerasClassifier(build_fn=createNetowrkGridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epochs': [80, 100],\n",
    "    'batch_size': [5, 10],\n",
    "    'units': [30, 20],\n",
    "    'dropout': [0.1, 0.20],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'loss': ['binary_crossentropy', 'mean_squared_error'],\n",
    "    'activation': ['tanh', 'relu']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsearch = GridSearchCV(estimator=clf_gridsearch, param_grid=params,\n",
    "                        scoring='accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = gdsearch.fit(X=df, y=classes) # This will take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = fit.best_params_\n",
    "best_acc = fit.best_score_\n",
    "\n",
    "best_params, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 3.0 - Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:, 0:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEnc = LabelEncoder()\n",
    "\n",
    "classes = labelEnc.fit_transform(df.iloc[:, 4].values)\n",
    "classes_dummy = np_utils.to_categorical(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(data, classes_dummy, test_size=0.25, random_state=0)\n",
    "\n",
    "f'trainX: {trainX.shape}, trainY:{trainY.shape}, testX: {testX.shape}, testY: {testY.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqNet():\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(units=4, activation='relu', input_dim=4))\n",
    "    clf.add(Dense(units=8, activation='relu'))\n",
    "    clf.add(Dense(units=8, activation='relu'))\n",
    "    clf.add(Dense(units=8, activation='relu'))\n",
    "    clf.add(Dense(units=3, activation='softmax'))\n",
    "    clf.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = seqNet()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x=trainX, y=trainY, epochs=1500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.evaluate(x=testX, y=testY)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (clf.predict(x=testX) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testY_argmax = [np.argmax(x) for x in testY] # returns the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred_argmax = [np.argmax(x) for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    confusion_matrix(y_true=_testY_argmax, y_pred=_pred_argmax),\n",
    "    accuracy_score(y_true=_testY_argmax, y_pred=_pred_argmax)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 3.1 - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifier(build_fn=seqNet, epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_score(estimator=clf, X=data, y=classes, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.mean(), res.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 4.0 - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/autos.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['dateCrawled', 'dateCreated', 'nrOfPictures', 'postalCode',\n",
    "                'name', 'lastSeen', 'seller', 'offerType'],\n",
    "        axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.price == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.price > 350000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = df[df.price <= 10]\n",
    "i1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2 = df[df.price > 350000]\n",
    "i2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.price > 10]\n",
    "df = df[df.price < 350000]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df[pd.isnull(df['vehicleType'])].shape[0],\n",
    "    df[pd.isnull(df['gearbox'])].shape[0],\n",
    "    df[pd.isnull(df['model'])].shape[0],\n",
    "    df[pd.isnull(df['fuelType'])].shape[0],\n",
    "    df[pd.isnull(df['notRepairedDamage'])].shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vehicleType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gearbox'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fuelType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['notRepairedDamage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = {\n",
    "    'vehicleType': 'limousine',\n",
    "    'gearbox': 'manuell',\n",
    "    'model': 'golf',\n",
    "    'fuelType': 'benzin',\n",
    "    'notRepairedDamage': 'nein'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value=fill, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = df.iloc[:, 1:13].values\n",
    "target = df.iloc[:, 0].values\n",
    "\n",
    "f'features: {feat.shape}, target: {target.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEnc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat[:, 0] = labelEnc.fit_transform(feat[:, 0])\n",
    "feat[:, 1] = labelEnc.fit_transform(feat[:, 1])\n",
    "feat[:, 3] = labelEnc.fit_transform(feat[:, 3])\n",
    "feat[:, 5] = labelEnc.fit_transform(feat[:, 5])\n",
    "feat[:, 8] = labelEnc.fit_transform(feat[:, 8])\n",
    "feat[:, 9] = labelEnc.fit_transform(feat[:, 9])\n",
    "feat[:, 10] = labelEnc.fit_transform(feat[:, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotEnc = OneHotEncoder(categorical_features=[0, 1, 3, 5, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = onehotEnc.fit_transform(feat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.add(Dense(units=316, activation='relu', input_dim=316))\n",
    "reg.add(Dense(units=158, activation='relu'))\n",
    "reg.add(Dense(units=32, activation='relu'))\n",
    "reg.add(Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.compile(optimizer='adam', loss='mean_absolute_error', metrics='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg.fit(x=feat, y=target, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = reg.predict(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean = target.mean()\n",
    "p_mean = pred.mean()\n",
    "\n",
    "f'real mean: {t_mean}, pred mean: {p_mean}, abs distance: {abs(t_mean - p_mean)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 4.1 - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNet():\n",
    "    reg = Sequential()\n",
    "    reg.add(Dense(units=158, activation='relu', input_dim=316))\n",
    "    reg.add(Dense(units=158, activation='relu'))\n",
    "    reg.add(Dense(units=1, activation='linear'))\n",
    "    reg.compile(optimizer='adam', loss='mean_absolute_error', metrics='mean_absolute_error')\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = KerasRegressor(build_fn=createNet, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = cross_val_score(estimator=reg, X=feat, y=target, cv=10, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 5 - Multi regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/games.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans(df).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['Name', 'Other_Sales', 'Global_Sales', 'Developer'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['NA_Sales'] > 1]\n",
    "df = df.loc[df['EU_Sales'] > 1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:, [0, 1, 2, 3, 7, 8, 9, 10, 11]].values\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sells_na = df.iloc[:, 4].values\n",
    "sells_eu = df.iloc[:, 5].values\n",
    "sells_jp = df.iloc[:, 6].values\n",
    "\n",
    "f'Sells: NA = {sells_na.shape[0]}, EU = {sells_eu.shape[0]}, JP = {sells_jp.shape[0]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:, 0] = labelEncoder.fit_transform(features[:, 0])\n",
    "features[:, 2] = labelEncoder.fit_transform(features[:, 2])\n",
    "features[:, 3] = labelEncoder.fit_transform(features[:, 3])\n",
    "features[:, 8] = labelEncoder.fit_transform(features[:, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot = OneHotEncoder(categorical_features=[0, 2, 3, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = oneHot.fit_transform(features).toarray()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 5.1 - Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_layer = Input(shape=(61,), name='input')\n",
    "# hidden\n",
    "hidden_1 = Dense(units=32, activation='sigmoid', name='h1')(input_layer)\n",
    "hidden_2 = Dense(units=32, activation='sigmoid', name='h2')(hidden_1)\n",
    "# output\n",
    "output_layer_1 = Dense(units=1, activation='linear', name='o1')(hidden_2)\n",
    "output_layer_2 = Dense(units=1, activation='linear', name='o2')(hidden_2)\n",
    "output_layer_3 = Dense(units=1, activation='linear', name='o3')(hidden_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Model(inputs=input_layer, outputs=[output_layer_1, output_layer_2, output_layer_3])\n",
    "reg.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(x=features, y=[sells_na, sells_eu, sells_jp], epochs=10000, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_na, pred_eu, pred_jp = reg.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_na.mean(), pred_eu.mean(), pred_jp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Mean distances: \\\n",
    "NA = {abs(sells_na.mean() - pred_na.mean())}, \\\n",
    "EU = {abs(sells_eu.mean() - pred_eu.mean())}, \\\n",
    "JP = {abs(sells_jp.mean() - pred_jp.mean())}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 6.0 - Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainx, trainy), (testx, testy) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx.shape, trainy.shape, testx.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(label=f'Classe: {trainy[123]}')\n",
    "plt.imshow(X=trainx[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = trainx.reshape(trainx.shape[0], 28, 28, 1)\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = testx.reshape(testx.shape[0], 28, 28, 1)\n",
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train.astype('float32')\n",
    "features_test = features_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train/255\n",
    "features_test = features_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = np_utils.to_categorical(y=trainy, num_classes=10)\n",
    "target_test = np_utils.to_categorical(y=testy, num_classes=10)\n",
    "\n",
    "target_train.shape, target_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 6.1 - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output shape: $floor(\\frac{n-f+2p}{s}+1) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc output shape\n",
    "def outputShape(n, f, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    This works for square matrices: n == height == width\n",
    "    \"\"\"\n",
    "    return int(((n - f + 2 * padding ) / stride) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(verbose=True):\n",
    "    clf = Sequential()\n",
    "    clf.add(layer=Conv2D(filters=32, kernel_size=(3, 3), activation='relu', name='conv',\n",
    "                    input_shape=(28, 28, 1))) #outputs 26x26\n",
    "    clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool')) #outputs 13x13\n",
    "    clf.add(layer=Flatten(name='flatten'))\n",
    "    clf.add(layer=Dense(units=128, activation='relu', name='dense'))\n",
    "    clf.add(layer=Dense(units=10, activation='softmax', name='softmax'))\n",
    "    clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    if verbose:\n",
    "        print(clf.summary())\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x=features_train, y=target_train, batch_size=100, epochs=10,\n",
    "        validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.evaluate(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 6.2 - Model upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_upgraded(verbose=True):\n",
    "    clf = Sequential()\n",
    "    clf.add(layer=Conv2D(filters=32, kernel_size=(3, 3), activation='relu', name='conv1',\n",
    "                    input_shape=(28, 28, 1)))\n",
    "    clf.add(BatchNormalization(name='batchnorm1'))\n",
    "    clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool1'))\n",
    "    clf.add(layer=Conv2D(filters=32, kernel_size=(3, 3), activation='relu', name='conv2'))\n",
    "    clf.add(BatchNormalization(name='batchnorm2'))\n",
    "    clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool2'))\n",
    "    clf.add(layer=Flatten(name='flatten'))\n",
    "    clf.add(layer=Dense(units=128, activation='relu', name='dense1'))\n",
    "    clf.add(Dropout(rate=0.2, name='dropout1'))\n",
    "    clf.add(layer=Dense(units=64, activation='relu', name='dense2'))\n",
    "    clf.add(Dropout(rate=0.2, name='dropout2'))\n",
    "    clf.add(layer=Dense(units=10, activation='softmax', name='softmax'))\n",
    "    clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    if verbose:\n",
    "        print(clf.summary())\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model_upgraded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x=features_train, y=target_train, batch_size=100, epochs=10,\n",
    "        validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 6.3 - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for itrain, itest in kfold.split(X=features_train, y=np.zeros(shape=(target_train.shape[0], 1))):\n",
    "    clf = model_upgraded(verbose=False)\n",
    "    clf.fit(x=features_train[itrain], y=target_train[itrain], batch_size=100, epochs=5)\n",
    "    prec = clf.evaluate(features_train[itest], target_train[itest])\n",
    "    results.append(prec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(results).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 6.4 - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model_upgraded(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imggen_train = ImageDataGenerator(rotation_range=7, height_shift_range=0.07,\n",
    "                            shear_range=0.2, zoom_range=0.2)\n",
    "\n",
    "imggen_test = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_gen = imggen_train.flow(x=features_train, y=target_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_gen = imggen_test.flow(x=features_test, y=target_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.fit_generator(generator=features_train_gen, steps_per_epoch=60000/128, epochs=5,\n",
    "                 validation_data=features_test_gen, validation_steps=10000/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.evaluate(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 6.5 - CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain, ytrain), (xtest, ytest) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xtrain[666])\n",
    "plt.title('Classe '+ str(ytrain[666]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape(xtrain.shape[0], 32, 32, 3)\n",
    "xtest = xtest.reshape(xtest.shape[0], 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.astype('float32')\n",
    "xtest = xtest.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain /= 255\n",
    "xtest /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = np_utils.to_categorical(ytrain, 10)\n",
    "ytest = np_utils.to_categorical(ytest, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelCifar10():\n",
    "    clf = Sequential()\n",
    "    clf.add(layer=Conv2D(filters=32, kernel_size=(3, 3), activation='relu', name='conv1',\n",
    "                    input_shape=(32, 32, 3)))\n",
    "    clf.add(BatchNormalization(name='batchnorm1'))\n",
    "    clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool1'))\n",
    "    clf.add(layer=Conv2D(filters=32, kernel_size=(3, 3), activation='relu', name='conv2'))\n",
    "    clf.add(BatchNormalization(name='batchnorm2'))\n",
    "    clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool2'))\n",
    "    clf.add(layer=Flatten(name='flatten'))\n",
    "    clf.add(layer=Dense(units=128, activation='relu', name='dense1'))\n",
    "    clf.add(Dropout(rate=0.2, name='dropout1'))\n",
    "    clf.add(layer=Dense(units=128, activation='relu', name='dense2'))\n",
    "    clf.add(Dropout(rate=0.2, name='dropout2'))\n",
    "    clf.add(layer=Dense(units=10, activation='softmax', name='softmax'))\n",
    "    clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "#     print(clf.summary())\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = modelCifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(x=xtrain, y=ytrain, batch_size=32, epochs=5, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 7.0 - Cats & dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'datasets/catsAndDogs/training_set/'\n",
    "test_dir = 'datasets/catsAndDogs/test_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catDogClf(verbose=False):\n",
    "    clf = Sequential(name='catDogClf')\n",
    "\n",
    "    clf.add(layer=Conv2D(filters=64, kernel_size=(3, 3), activation='relu', name='conv1',\n",
    "                    input_shape=(128, 128, 3)))\n",
    "    clf.add(BatchNormalization(name='batchnorm1'))\n",
    "    clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool1'))\n",
    "\n",
    "    clf.add(layer=Conv2D(filters=128, kernel_size=(3, 3), activation='relu', name='conv2'))\n",
    "    clf.add(BatchNormalization(name='batchnorm2'))\n",
    "    clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool2'))\n",
    "    \n",
    "#     clf.add(layer=Conv2D(filters=64, kernel_size=(3, 3), activation='relu', name='conv3'))\n",
    "#     clf.add(BatchNormalization(name='batchnorm3'))\n",
    "#     clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool3'))\n",
    "    \n",
    "#     clf.add(layer=Conv2D(filters=128, kernel_size=(3, 3), activation='relu', name='conv4'))\n",
    "#     clf.add(BatchNormalization(name='batchnorm4'))\n",
    "#     clf.add(layer=MaxPooling2D(pool_size=(2, 2), name='maxpool4'))\n",
    "\n",
    "    clf.add(layer=Flatten(name='flatten'))\n",
    "\n",
    "    clf.add(layer=Dense(units=32, activation='relu', name='dense1'))\n",
    "    clf.add(Dropout(rate=0.2, name='dropout1'))\n",
    "\n",
    "#     clf.add(layer=Dense(units=64, activation='relu', name='dense2'))\n",
    "#     clf.add(Dropout(rate=0.2, name='dropout2'))\n",
    "\n",
    "    clf.add(layer=Dense(units=1, activation='sigmoid', name='sigmoid'))\n",
    "\n",
    "    clf.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
    "    \n",
    "    if verbose:\n",
    "        print(clf.summary())\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = catDogClf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imggen_train = ImageDataGenerator(rescale=1./255, rotation_range=7, horizontal_flip=True,\n",
    "                                 shear_range=0.2, height_shift_range=0.07, zoom_range=0.2)\n",
    "imggen_test = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It should contain one subdirectory per class.\n",
    "train_data = imggen_train.flow_from_directory(directory=train_dir, target_size=(128, 128),\n",
    "                                              batch_size=8, class_mode='binary')\n",
    "test_data = imggen_test.flow_from_directory(directory=test_dir, target_size=(128, 128),\n",
    "                                              batch_size=8, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit_generator(generator=train_data, steps_per_epoch=4000/8, epochs=10,\n",
    "                  validation_data=test_data, validation_steps=1000/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = \"apolinho2.jpeg bruce1.jpeg not_bruce.jpeg \\\n",
    "                apolinho3.jpeg german.jpg\".split()\n",
    "\n",
    "for i in test_images:\n",
    "    test_img_file = f'datasets/catsAndDogs/{i}'\n",
    "    test_img = image.load_img(path=test_img_file, target_size=(128, 128))\n",
    "    plt.imshow(test_img)\n",
    "\n",
    "    test_img = image.img_to_array(img=test_img)\n",
    "    test_img /= 255\n",
    "    test_img = np.expand_dims(a=test_img, axis=0)\n",
    "    test_img.shape\n",
    "\n",
    "    pred = clf.predict(x=test_img)\n",
    "    print(pred)\n",
    "\n",
    "    if pred[0] < 0.5:\n",
    "        print('Cat')\n",
    "    else:\n",
    "        print('Dog')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 8.1 - Homer and Bart - Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath_or_buffer='datasets/personagens.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:, 0:6].values\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.iloc[:, 6].values\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "target = labelencoder.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(features, target,\n",
    "                                                test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain.shape, ytrain.shape, xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpsonsDense():\n",
    "    clf = Sequential()\n",
    "    clf.add(layer=Dense(units=32, input_dim=6, activation='relu'))\n",
    "    clf.add(layer=Dense(units=64, activation='relu'))\n",
    "    clf.add(layer=Dense(units=32, activation='relu'))\n",
    "    clf.add(layer=Dense(units=1, activation='sigmoid'))\n",
    "    clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return clf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = simpsonsDense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x=xtrain, y=ytrain, batch_size=10, epochs=500,\n",
    "        validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 8.1 - Homer and Bart - Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale = 1./255, rotation_range=7, \n",
    "                                         horizontal_flip = True, shear_range=0.2,\n",
    "                                         height_shift_range=0.07, zoom_range=0.2)\n",
    "test_gen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_gen.flow_from_directory(directory='datasets/dataset_personagens/training_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=10,\n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_gen.flow_from_directory(directory='datasets/dataset_personagens/test_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=10,\n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpsonsCONV():\n",
    "    clf = Sequential()\n",
    "    \n",
    "    clf.add(layer=Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "    clf.add(layer=MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    clf.add(layer=Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "    clf.add(layer=MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    clf.add(layer=Flatten())\n",
    "    \n",
    "    clf.add(layer=Dense(units=32, activation='relu'))\n",
    "    clf.add(layer=Dense(units=64, activation='relu'))\n",
    "    clf.add(layer=Dense(units=32, activation='relu'))\n",
    "    clf.add(layer=Dense(units=1, activation='sigmoid'))\n",
    "    clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = simpsonsCONV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit_generator(generator=train_data, steps_per_epoch=196/10, epochs=500,\n",
    "                         validation_data=test_data, validation_steps=73/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 9.0 - RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 9.1 - Stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('datasets/petr4_treinamento.csv')\n",
    "dftest = pd.read_csv('datasets/petr4_teste.csv')\n",
    "\n",
    "dftrain.shape, dftest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.dropna(inplace=True)\n",
    "dftrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain_norm = scaler.fit_transform(dftrain.iloc[:, 1:2])\n",
    "dftrain_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall have the network use 90 days previous prices to predict the future price, therefore:\n",
    "\n",
    "$f([x_0, x_1, ... x_{90}]) = x_{91} = y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = []\n",
    "real_price = []\n",
    "\n",
    "for i in range(90, 1242):\n",
    "    prev.append(dftrain_norm[i-90:i, 0])\n",
    "    real_price.append(dftrain_norm[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = np.array(prev)\n",
    "real_price = np.array(real_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev.shape, real_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev[1][89] == real_price[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = np.reshape(a=prev, newshape=(prev.shape[0], prev.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM():\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    clf = Sequential()\n",
    "\n",
    "    clf.add(layer=LSTM(units=100, return_sequences=True, input_shape=(prev.shape[1], 1)))\n",
    "    clf.add(layer=Dropout(rate=0.3))\n",
    "\n",
    "    clf.add(layer=LSTM(units=50, return_sequences=True))\n",
    "    clf.add(layer=Dropout(rate=0.3))\n",
    "\n",
    "    clf.add(layer=LSTM(units=50))\n",
    "    clf.add(layer=Dropout(rate=0.3))\n",
    "\n",
    "    clf.add(layer=Dense(units=1, activation='linear'))\n",
    "\n",
    "    clf.compile(optimizer='rmsprop',\n",
    "                loss='mean_squared_error',\n",
    "                metrics='mean_absolute_error')\n",
    "    \n",
    "    print(clf.summary())\n",
    "\n",
    "    return clf\n",
    "\n",
    "clf = model_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(x=prev, y=real_price, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 9.1.2 - Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dftest.iloc[:, 1:2].values\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.concat(objs=(dftrain['Open'], dftest['Open']), axis=0)\n",
    "fulldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = fulldf[len(fulldf) - len(dftest) - 90:].values.reshape(-1, 1)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_norm = scaler.transform(input_data)\n",
    "input_data_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = []\n",
    "for i in range(90, 112):\n",
    "    xtest.append(input_data_norm[i-90:i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.array(xtest)\n",
    "xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.reshape(xtest, (xtest.shape[0], xtest.shape[1], 1))\n",
    "xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(xtest)\n",
    "pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = scaler.inverse_transform(pred)\n",
    "pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Abs mean error (real - pred) = {abs(test_data.mean() - pred.mean())}' # =< 0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 9.1.3 - Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_data, color='red', label='real')\n",
    "plt.plot(pred, color='blue', label='pred')\n",
    "plt.title('stock prices')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('price (yahoo finance)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 9.2 - Stock prices (multiple input variables) and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('datasets/petr4_treinamento.csv')\n",
    "dftest = pd.read_csv('datasets/petr4_teste.csv')\n",
    "\n",
    "dftrain.shape, dftest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.dropna(inplace=True)\n",
    "dftrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dftrain.iloc[:, 1:7].values\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmaxScaler = MinMaxScaler(feature_range=(0,1))\n",
    "train_data_scaled = mmaxScaler.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_scaled = []\n",
    "train_target_scaled = []\n",
    "for i in range(90, train_data.shape[0]):\n",
    "    train_features_scaled.append(train_data_scaled[i-90:i, 0:6])\n",
    "    train_target_scaled.append(train_data_scaled[i, 0])\n",
    "\n",
    "len(train_features_scaled), len(train_target_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_scaled[1][89] == train_target_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_scaled = np.array(train_features_scaled)\n",
    "train_target_scaled = np.array(train_target_scaled)\n",
    "\n",
    "train_features_scaled.shape, train_features_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input is of shape (90, 6), i.e.: 90 days worth of 6 attributes, **for each** input register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_LSTM():\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    clf = Sequential()\n",
    "\n",
    "    clf.add(layer=LSTM(units=100, return_sequences=True,\n",
    "                       input_shape=(train_features_scaled.shape[1], train_features_scaled.shape[2])))\n",
    "\n",
    "    clf.add(layer=Dropout(rate=0.25))\n",
    "\n",
    "    clf.add(layer=LSTM(units=50, return_sequences=True))\n",
    "    clf.add(layer=Dropout(rate=0.25))\n",
    "\n",
    "    clf.add(layer=LSTM(units=100, return_sequences=True))\n",
    "    clf.add(layer=Dropout(rate=0.25))\n",
    "            \n",
    "    clf.add(layer=(Dense(units=1, activation='linear')))\n",
    "\n",
    "    clf.compile(optimizer='rmsprop', loss='mse', metrics='mae')\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "clf = clf_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', min_delta=1e-10, patience=5, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='sess9_2-weights.h5', save_best_only=True, monitor='loss',\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(x=train_features_scaled, y=train_target_scaled,\n",
    "        epochs=10, batch_size=10, callbacks=[es, rlr, mcp],\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 9.2.2 - Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.concat(objs=(dftrain, dftest), axis=0)\n",
    "fulldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf.drop(labels=['Date'], axis=1, inplace=True)\n",
    "fulldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = fulldf[len(fulldf) - len(dftest) - 90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_scaled = mmaxScaler.transform(test_features)\n",
    "test_features_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "for i in range(90, test_features_scaled.shape[0]):\n",
    "    test_features.append(test_features_scaled[i-90:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array(test_features)\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(test_features)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on trainig data, but choose only the 'Price' column\n",
    "mmaxScaler_test = MinMaxScaler(feature_range=(0,1))\n",
    "mmaxScaler_test.fit_transform(train_data[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mmaxScaler_test.inverse_transform(pred[:, 0])\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = dftest.iloc[:, 1:2].values\n",
    "test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_target, color='red', label='real')\n",
    "plt.plot(pred, color='blue', label='pred')\n",
    "plt.title('stock prices')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('price (yahoo finance)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 9.3 - Stock prices (multiple outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close  \\\n",
       "0  2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1  2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2  2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3  2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4  2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "\n",
       "       Volume  \n",
       "0  30182600.0  \n",
       "1  30552600.0  \n",
       "2  36141000.0  \n",
       "3  28069600.0  \n",
       "4  29091300.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain = pd.read_csv('datasets/petr4_treinamento.csv')\n",
    "\n",
    "dftrain.dropna(inplace=True)\n",
    "\n",
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1242, 1), (1242, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_open = dftrain.iloc[:, 1:2].values #['Open'].values\n",
    "train_features_high = dftrain.iloc[:, 2:3].values #['High'].values\n",
    "\n",
    "train_features_open.shape, train_features_high.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_open = MinMaxScaler(feature_range=(0,1))\n",
    "# train_features_open_scaled = scaler_open.fit_transform(train_features_open)\n",
    "# train_features_open_scaled[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_high = MinMaxScaler(feature_range=(0,1))\n",
    "# train_features_high_scaled = scaler_high.fit_transform(train_features_high)\n",
    "# train_features_high_scaled[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_range = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1152, 90), (1152,), (1152,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = np.array(\n",
    "    [\n",
    "        train_features_open[i-data_range:i, 0] \n",
    "        for i in range(data_range, train_features_open.shape[0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_output_open = train_features_open[data_range:, 0]\n",
    "train_output_high = train_features_high[data_range:, 0]\n",
    "\n",
    "train_input.shape, train_output_open.shape, train_output_high.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 19.91)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[1][data_range-1] == train_output_open[0], train_output_open[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 90, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = train_input.reshape((train_input.shape[0], train_input.shape[1], 1))\n",
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output = np.column_stack((train_output_open, train_output_high))\n",
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp = ModelCheckpoint(filepath='sess9_2-weights.h5', save_best_only=True, monitor='loss',\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 69.6392 - mean_absolute_error: 7.1507\n",
      "Epoch 00001: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 69.6392 - mean_absolute_error: 7.1507\n",
      "Epoch 2/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 18.9749 - mean_absolute_error: 3.4467\n",
      "Epoch 00002: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 18.9749 - mean_absolute_error: 3.4467\n",
      "Epoch 3/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 7.3428 - mean_absolute_error: 2.0778\n",
      "Epoch 00003: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 7.3428 - mean_absolute_error: 2.0778\n",
      "Epoch 4/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 4.8980 - mean_absolute_error: 1.7179\n",
      "Epoch 00004: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 56ms/step - loss: 4.8980 - mean_absolute_error: 1.7179\n",
      "Epoch 5/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 4.1889 - mean_absolute_error: 1.5983\n",
      "Epoch 00005: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 4.1889 - mean_absolute_error: 1.5983\n",
      "Epoch 6/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 4.2360 - mean_absolute_error: 1.5678\n",
      "Epoch 00006: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 4.2598 - mean_absolute_error: 1.5711\n",
      "Epoch 7/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 3.8048 - mean_absolute_error: 1.5156\n",
      "Epoch 00007: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 54ms/step - loss: 3.8234 - mean_absolute_error: 1.5187\n",
      "Epoch 8/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 3.3602 - mean_absolute_error: 1.4208\n",
      "Epoch 00008: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 3.3551 - mean_absolute_error: 1.4194\n",
      "Epoch 9/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 3.3313 - mean_absolute_error: 1.4256\n",
      "Epoch 00009: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 3.3270 - mean_absolute_error: 1.4246\n",
      "Epoch 10/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 3.1976 - mean_absolute_error: 1.3675\n",
      "Epoch 00010: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 56ms/step - loss: 3.1976 - mean_absolute_error: 1.3675\n",
      "Epoch 11/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 3.0911 - mean_absolute_error: 1.3751\n",
      "Epoch 00011: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 60ms/step - loss: 3.0890 - mean_absolute_error: 1.3748\n",
      "Epoch 12/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 3.1944 - mean_absolute_error: 1.3912\n",
      "Epoch 00012: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 3.1944 - mean_absolute_error: 1.3912\n",
      "Epoch 13/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 3.1755 - mean_absolute_error: 1.3773\n",
      "Epoch 00013: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 53ms/step - loss: 3.1710 - mean_absolute_error: 1.3761\n",
      "Epoch 14/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.6761 - mean_absolute_error: 1.2725\n",
      "Epoch 00014: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 54ms/step - loss: 2.6764 - mean_absolute_error: 1.2727\n",
      "Epoch 15/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.8446 - mean_absolute_error: 1.3192\n",
      "Epoch 00015: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 53ms/step - loss: 2.8413 - mean_absolute_error: 1.3181\n",
      "Epoch 16/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.9151 - mean_absolute_error: 1.3156\n",
      "Epoch 00016: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.9151 - mean_absolute_error: 1.3156\n",
      "Epoch 17/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.8091 - mean_absolute_error: 1.2826\n",
      "Epoch 00017: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 2.8091 - mean_absolute_error: 1.2826\n",
      "Epoch 18/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.6655 - mean_absolute_error: 1.2759\n",
      "Epoch 00018: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 58ms/step - loss: 2.6655 - mean_absolute_error: 1.2759\n",
      "Epoch 19/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.6733 - mean_absolute_error: 1.2834\n",
      "Epoch 00019: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 56ms/step - loss: 2.6733 - mean_absolute_error: 1.2834\n",
      "Epoch 20/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.6580 - mean_absolute_error: 1.2610\n",
      "Epoch 00020: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 56ms/step - loss: 2.6580 - mean_absolute_error: 1.2610\n",
      "Epoch 21/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.4073 - mean_absolute_error: 1.2027\n",
      "Epoch 00021: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 2.4073 - mean_absolute_error: 1.2027\n",
      "Epoch 22/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.6054 - mean_absolute_error: 1.2454\n",
      "Epoch 00022: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 56ms/step - loss: 2.6054 - mean_absolute_error: 1.2454\n",
      "Epoch 23/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.4903 - mean_absolute_error: 1.2104\n",
      "Epoch 00023: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 56ms/step - loss: 2.4903 - mean_absolute_error: 1.2104\n",
      "Epoch 24/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.5821 - mean_absolute_error: 1.2464\n",
      "Epoch 00024: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 56ms/step - loss: 2.5821 - mean_absolute_error: 1.2464\n",
      "Epoch 25/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.4470 - mean_absolute_error: 1.2209\n",
      "Epoch 00025: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 56ms/step - loss: 2.4470 - mean_absolute_error: 1.2209\n",
      "Epoch 26/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.4606 - mean_absolute_error: 1.2170\n",
      "Epoch 00026: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 56ms/step - loss: 2.4606 - mean_absolute_error: 1.2170\n",
      "Epoch 27/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.4750 - mean_absolute_error: 1.2231\n",
      "Epoch 00027: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 58ms/step - loss: 2.4750 - mean_absolute_error: 1.2231\n",
      "Epoch 28/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.6405 - mean_absolute_error: 1.2726\n",
      "Epoch 00028: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 59ms/step - loss: 2.6405 - mean_absolute_error: 1.2726\n",
      "Epoch 29/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.4422 - mean_absolute_error: 1.2047\n",
      "Epoch 00029: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.4383 - mean_absolute_error: 1.2032\n",
      "Epoch 30/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.4577 - mean_absolute_error: 1.2053\n",
      "Epoch 00030: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.4577 - mean_absolute_error: 1.2053\n",
      "Epoch 31/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.1430 - mean_absolute_error: 1.1343\n",
      "Epoch 00031: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.1430 - mean_absolute_error: 1.1343\n",
      "Epoch 32/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.3560 - mean_absolute_error: 1.1969\n",
      "Epoch 00032: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.3542 - mean_absolute_error: 1.1967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.1865 - mean_absolute_error: 1.1649\n",
      "Epoch 00033: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 54ms/step - loss: 2.1832 - mean_absolute_error: 1.1636\n",
      "Epoch 34/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.5048 - mean_absolute_error: 1.2200\n",
      "Epoch 00034: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 56ms/step - loss: 2.5048 - mean_absolute_error: 1.2200\n",
      "Epoch 35/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.1377 - mean_absolute_error: 1.1482\n",
      "Epoch 00035: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 54ms/step - loss: 2.1447 - mean_absolute_error: 1.1502\n",
      "Epoch 36/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.3500 - mean_absolute_error: 1.1770\n",
      "Epoch 00036: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 2.3500 - mean_absolute_error: 1.1770\n",
      "Epoch 37/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.3550 - mean_absolute_error: 1.1877\n",
      "Epoch 00037: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.3783 - mean_absolute_error: 1.1923\n",
      "Epoch 38/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.4214 - mean_absolute_error: 1.1897\n",
      "Epoch 00038: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.4196 - mean_absolute_error: 1.1897\n",
      "Epoch 39/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.2386 - mean_absolute_error: 1.1483\n",
      "Epoch 00039: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.2386 - mean_absolute_error: 1.1483\n",
      "Epoch 40/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.3294 - mean_absolute_error: 1.1813\n",
      "Epoch 00040: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.3294 - mean_absolute_error: 1.1813\n",
      "Epoch 41/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.2762 - mean_absolute_error: 1.1733\n",
      "Epoch 00041: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.2762 - mean_absolute_error: 1.1733\n",
      "Epoch 42/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.0956 - mean_absolute_error: 1.1246\n",
      "Epoch 00042: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 54ms/step - loss: 2.0922 - mean_absolute_error: 1.1231\n",
      "Epoch 43/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.2836 - mean_absolute_error: 1.1823\n",
      "Epoch 00043: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.2836 - mean_absolute_error: 1.1823\n",
      "Epoch 44/100\n",
      "115/116 [============================>.] - ETA: 0s - loss: 2.2311 - mean_absolute_error: 1.1457\n",
      "Epoch 00044: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 2.2298 - mean_absolute_error: 1.1455\n",
      "Epoch 45/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.1878 - mean_absolute_error: 1.1355\n",
      "Epoch 00045: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 2.1878 - mean_absolute_error: 1.1355\n",
      "Epoch 46/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.1139 - mean_absolute_error: 1.1100\n",
      "Epoch 00046: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 6s 55ms/step - loss: 2.1139 - mean_absolute_error: 1.1100\n",
      "Epoch 47/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.2817 - mean_absolute_error: 1.1692\n",
      "Epoch 00047: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 56ms/step - loss: 2.2817 - mean_absolute_error: 1.1692\n",
      "Epoch 48/100\n",
      "116/116 [==============================] - ETA: 0s - loss: 2.0191 - mean_absolute_error: 1.0906\n",
      "Epoch 00048: loss did not improve from 1.65228\n",
      "116/116 [==============================] - 7s 57ms/step - loss: 2.0191 - mean_absolute_error: 1.0906\n",
      "Epoch 49/100\n",
      "  8/116 [=>............................] - ETA: 5s - loss: 2.6060 - mean_absolute_error: 1.2350"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-af38b56c4883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmcp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def clf_LSTM():\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    clf = Sequential()\n",
    "    \n",
    "    clf.add(LSTM(units = 100, return_sequences = True,\n",
    "                 input_shape=(train_input.shape[1], 1)))\n",
    "    clf.add(Dropout(0.3))\n",
    "\n",
    "    clf.add(LSTM(units = 50, return_sequences = True))\n",
    "    clf.add(Dropout(0.3))\n",
    "\n",
    "    clf.add(LSTM(units = 50, return_sequences = True))\n",
    "    clf.add(Dropout(0.3))\n",
    "\n",
    "    clf.add(LSTM(units = 50))\n",
    "    clf.add(Dropout(0.3))\n",
    "\n",
    "    clf.add(Dense(units = 2, activation = 'linear'))\n",
    "\n",
    "    clf.compile(optimizer = 'rmsprop', loss = 'mean_squared_error',\n",
    "                      metrics = ['mean_absolute_error'])\n",
    "    return clf\n",
    "\n",
    "\n",
    "clf = clf_LSTM()\n",
    "\n",
    "clf.fit(x=train_input, y=train_output, epochs=100, batch_size=10, callbacks=[mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.read_csv('datasets/petr4_teste.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_open = dftest.iloc[:, 1:2].values\n",
    "target_high = dftest.iloc[:, 2:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1264,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulldf = pd.concat((dftrain['Open'], dftest['Open']), axis=0)\n",
    "fulldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = fulldf[len(fulldf) - len(dftest) - data_range:].values.reshape(-1, 1)\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = np.array(\n",
    "    [\n",
    "        test_features[i-data_range:i, 0] \n",
    "        for i in range(data_range, test_features.shape[0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_input = test_input.reshape((test_input.shape[0], test_input.shape[1], 1))\n",
    "        \n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 9.3.2 - Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(x=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Distance open price: {abs(target_open.mean() - pred[:, 0].mean())}' # >= 0.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Distance high price: {abs(target_high.mean() - pred[:, 1].mean())}' # >= 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,16))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(target_open, color='red', label='actual open')\n",
    "plt.plot(pred[:, 0], color='blue', label='pred open')\n",
    "plt.title('stock prices')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('price (yahoo finance)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(target_high, color='purple', label='actual high')\n",
    "plt.plot(pred[:, 1], color='green', label='pred high')\n",
    "plt.title('stock prices')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('price (yahoo finance)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 10.0 - Self Organizing Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
